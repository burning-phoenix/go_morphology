{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# 02 - Train Matryoshka SAE\n",
        "\n",
        "This notebook:\n",
        "1. Loads extracted activations from blocks 5, 20, 35\n",
        "2. Trains MSAE with hierarchical k levels [16, 32, 64, 128]\n",
        "3. Tracks reconstruction R² at each k level\n",
        "4. Saves trained models and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "hyei89e0j6k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyei89e0j6k",
        "outputId": "27a6010d-0dd3-4f25-e161-f00b46705ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Google Colab\n",
            "Mounted at /content/drive\n",
            "Drive mounted. Project root: /content/drive/MyDrive/chaos\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# COLAB SETUP - Run this cell first!\n",
        "# ============================================================================\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set up paths\n",
        "    DRIVE_ROOT = Path('/content/drive/MyDrive/chaos')\n",
        "    DRIVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Drive mounted. Project root: {DRIVE_ROOT}\")\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    DRIVE_ROOT = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-unzip",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-unzip",
        "outputId": "388b4539-9ca5-4e6c-9bd6-4959bf89a53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# COLAB: Install dependencies and upload src.zip\n",
        "# ============================================================================\n",
        "if IN_COLAB:\n",
        "    print(\"Installing dependencies...\")\n",
        "    !pip install -q torch>=2.0.0 h5py>=3.8.0 sgfmill>=1.1.0\n",
        "    !pip install -q matplotlib>=3.7.0 tqdm>=4.65.0\n",
        "    !pip install -q scikit-learn>=1.2.0 scipy>=1.10.0\n",
        "    print(\"Dependencies installed!\")\n",
        "\n",
        "    # Unzip src.zip\n",
        "    !unzip -n -q src.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-2",
        "outputId": "8c83ccaa-e181-4616-c603-1c64ab542a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: /content/drive/MyDrive/chaos\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add search path for 'src' module\n",
        "if IN_COLAB:\n",
        "    sys.path.insert(0, '/content')\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Configuration - Literature-validated hyperparameters\n",
        "# Sources:\n",
        "#   - Gao et al. \"Scaling and Evaluating Sparse Autoencoders\" (topk_sae_paper.md)\n",
        "#   - Multi-budget SAE (multi_budget_sae.md)\n",
        "#   - Matryoshka SAE (MatryoshkaSAE_paper.md)\n",
        "CONFIG = {\n",
        "    'output_dir': 'outputs',  # Will be overridden for Colab below\n",
        "\n",
        "    'block_indices': [5, 20, 35],  # Layers to train on\n",
        "\n",
        "    # MSAE architecture\n",
        "    'input_dim': 256,  # Leela Zero channel dimension\n",
        "    'expansion_factor': 16,  # Hidden = 256 * 16 = 4096 (paper: 8-32x)\n",
        "    'k_levels': [16, 32, 64, 128],  # Matryoshka sparsity levels\n",
        "    'weighting': 'uniform',  # 'uniform' or 'reverse' (paper: both work)\n",
        "\n",
        "    # Training hyperparameters (from Gao et al. & multi_budget_sae papers)\n",
        "    # Batch size: Paper uses 8096-131072. We use 4096 for memory efficiency\n",
        "    # LR: Paper suggests 1/sqrt(n) scaling. For n=4096, optimal ~0.0008-0.001\n",
        "    'batch_size': 4096,  # Paper: 8096, reduced for Colab T4 memory\n",
        "    'learning_rate': 8e-4,  # Paper: 0.0008 (multi_budget_sae Table 1)\n",
        "\n",
        "    # Option B: With 500K samples (vs 18M with flatten), increase epochs\n",
        "    # ~122 batches/epoch with 500K samples vs ~4400 with 18M\n",
        "    'epochs': 30,  # Increased from 15 to compensate for fewer samples\n",
        "    'val_split': 0.1,\n",
        "    'early_stopping_patience': 5,  # Increased patience for longer training\n",
        "\n",
        "    # Auxiliary loss for dead latent prevention\n",
        "    # Paper: aux_k=512, aux_coefficient=1/32\n",
        "    'aux_k': 512,\n",
        "    'aux_coefficient': 1/32,\n",
        "\n",
        "    # Optimizer settings (from topk_sae_paper Appendix A)\n",
        "    # Adam with beta1=0.9, beta2=0.999\n",
        "    # Epsilon very small: 6.25e-10 for large scale\n",
        "    'adam_beta1': 0.9,\n",
        "    'adam_beta2': 0.999,\n",
        "    'adam_eps': 1e-8,  # Standard for our scale\n",
        "\n",
        "    # Decoder normalization: True (paper: essential)\n",
        "    'normalize_decoder': True,\n",
        "\n",
        "    # CRITICAL: Shuffling is handled by DataLoader (shuffle=True)\n",
        "    # This prevents learning spurious order-dependent patterns\n",
        "    # See: Anthropic \"Engineering Challenges in Interpretability\"\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# COLAB: Configure paths for Drive storage\n",
        "# ============================================================================\n",
        "if IN_COLAB:\n",
        "    CONFIG['output_dir'] = str(DRIVE_ROOT)\n",
        "\n",
        "    # Ensure output directories exist\n",
        "    (DRIVE_ROOT / 'data').mkdir(parents=True, exist_ok=True)\n",
        "    (DRIVE_ROOT / 'data' / 'activations').mkdir(parents=True, exist_ok=True)\n",
        "    (DRIVE_ROOT / 'models').mkdir(parents=True, exist_ok=True)\n",
        "    (DRIVE_ROOT / 'results').mkdir(parents=True, exist_ok=True)\n",
        "    (DRIVE_ROOT / 'figures').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Output directory: {CONFIG['output_dir']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-3",
        "outputId": "9f03eb97-2a79-49fd-ca19-d735caee013c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Device selection\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
        "        return device\n",
        "    if torch.backends.mps.is_available():\n",
        "        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "        print(\"Using MPS (Apple Silicon)\")\n",
        "        return torch.device('mps')\n",
        "    print(\"Using CPU\")\n",
        "    return torch.device('cpu')\n",
        "\n",
        "device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-4",
      "metadata": {
        "id": "cell-4"
      },
      "outputs": [],
      "source": [
        "from src.models import MatryoshkaSAE, create_msae\n",
        "from src.training import MSAETrainer, create_activation_dataloader\n",
        "from src.utils import clear_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rhmnop2j7t",
      "metadata": {
        "id": "rhmnop2j7t"
      },
      "source": [
        "<cell_type>markdown</cell_type>## 0. Load Data (Colab: from Google Drive)\n",
        "\n",
        "If running on Colab, activations should be available at `DRIVE_ROOT/data/activations/`.\n",
        "\n",
        "**Expected Drive structure:**\n",
        "```\n",
        "MyDrive/chaos/\n",
        "└── data/\n",
        "    └── activations/\n",
        "        ├── block5/\n",
        "        ├── block20/\n",
        "        └── block35/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7pstfizr1p3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pstfizr1p3",
        "outputId": "317bdc59-aad9-4aa4-b2ee-eafbe523ccf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for activation data...\n",
            "  Block 5: 10 chunks found\n",
            "  Block 20: 10 chunks found\n",
            "  Block 35: 10 chunks found\n"
          ]
        }
      ],
      "source": [
        "# Verify activations are available\n",
        "from pathlib import Path\n",
        "\n",
        "activations_dir = Path(CONFIG['output_dir']) / 'data' / 'activations'\n",
        "\n",
        "print(\"Checking for activation data...\")\n",
        "for block_idx in CONFIG['block_indices']:\n",
        "    block_dir = activations_dir / f'block{block_idx}'\n",
        "    if block_dir.exists():\n",
        "        chunks = list(block_dir.glob('chunk_*.npy'))\n",
        "        print(f\"  Block {block_idx}: {len(chunks)} chunks found\")\n",
        "    else:\n",
        "        print(f\"  Block {block_idx}: NOT FOUND - run notebook 01 first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "source": [
        "## 1. Check Available Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cell-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-6",
        "outputId": "dd573580-275c-43d1-965b-a53c1546bca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available activation data:\n",
            "  Block 5: 10 chunks, stats: True\n",
            "  Block 20: 10 chunks, stats: True\n",
            "  Block 35: 10 chunks, stats: True\n"
          ]
        }
      ],
      "source": [
        "activations_dir = Path(CONFIG['output_dir']) / 'data' / 'activations'\n",
        "\n",
        "print(\"Available activation data:\")\n",
        "for block_idx in CONFIG['block_indices']:\n",
        "    block_dir = activations_dir / f'block{block_idx}'\n",
        "    if block_dir.exists():\n",
        "        chunks = list(block_dir.glob('chunk_*.npy'))\n",
        "        stats_file = block_dir / 'normalization_stats.npz'\n",
        "        print(f\"  Block {block_idx}: {len(chunks)} chunks, stats: {stats_file.exists()}\")\n",
        "    else:\n",
        "        print(f\"  Block {block_idx}: NOT FOUND\")\n",
        "        print(f\"    Run 01_setup_and_extraction.ipynb first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## 2. Train MSAE for Each Block\n",
        "\n",
        "We train separate MSAEs for each layer (blocks 5, 20, 35)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cell-8",
      "metadata": {
        "id": "cell-8"
      },
      "outputs": [],
      "source": [
        "def train_msae_for_block(block_idx: int) -> dict:\n",
        "    \"\"\"\n",
        "    Train MSAE for a single block.\n",
        "\n",
        "    Returns:\n",
        "        Training history and final metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training MSAE for Block {block_idx}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Load data from HDF5\n",
        "    h5_path = Path(CONFIG['output_dir']) / 'data' / 'activations.h5'\n",
        "    train_loader, val_loader, norm_stats = create_activation_dataloader(\n",
        "        activations_dir=str(activations_dir),\n",
        "        block_idx=block_idx,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        normalize=True,\n",
        "        val_split=CONFIG['val_split'],\n",
        "        h5_path=str(h5_path),\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = create_msae(\n",
        "        input_dim=CONFIG['input_dim'],\n",
        "        expansion_factor=CONFIG['expansion_factor'],\n",
        "        k_levels=CONFIG['k_levels'],\n",
        "        weighting=CONFIG['weighting'],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Create trainer with literature-validated optimizer settings\n",
        "    trainer = MSAETrainer(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        lr=CONFIG['learning_rate'],  # Paper: 0.0008 from multi_budget_sae\n",
        "        output_dir=CONFIG['output_dir'],\n",
        "        device=device,\n",
        "        checkpoint_every=5,\n",
        "        log_every=50,\n",
        "        adam_betas=(CONFIG.get('adam_beta1', 0.9), CONFIG.get('adam_beta2', 0.999)),\n",
        "        adam_eps=CONFIG.get('adam_eps', 6.25e-10),\n",
        "    )\n",
        "\n",
        "    # Train with early stopping (keeps best model automatically)\n",
        "    history = trainer.train(\n",
        "        epochs=CONFIG['epochs'],\n",
        "        early_stopping_patience=CONFIG['early_stopping_patience'],\n",
        "    )\n",
        "\n",
        "    # Load best model (not the last epoch's model)\n",
        "    best_model_path = Path(CONFIG['output_dir']) / 'models' / 'msae_best.pt'\n",
        "    if best_model_path.exists():\n",
        "        best_checkpoint = torch.load(best_model_path, map_location=device)\n",
        "        model.load_state_dict(best_checkpoint['model_state_dict'])\n",
        "        best_epoch = best_checkpoint.get('epoch', 'unknown')\n",
        "        print(f\"\\nLoaded best model from epoch {best_epoch}\")\n",
        "\n",
        "    # Save final model with normalization stats (using best weights)\n",
        "    final_path = Path(CONFIG['output_dir']) / 'models' / f'msae_block{block_idx}.pt'\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': {\n",
        "            'input_dim': CONFIG['input_dim'],\n",
        "            'hidden_dim': CONFIG['input_dim'] * CONFIG['expansion_factor'],\n",
        "            'k_levels': CONFIG['k_levels'],\n",
        "            'weighting': CONFIG['weighting'],\n",
        "        },\n",
        "        'normalization': {\n",
        "            'mean': norm_stats['mean'].numpy(),\n",
        "            'std': norm_stats['std'].numpy(),\n",
        "        },\n",
        "        'history': history,\n",
        "    }, final_path)\n",
        "    print(f\"Saved final model: {final_path}\")\n",
        "\n",
        "    # Extract final metrics (from best epoch)\n",
        "    # Find the best epoch index based on val_loss\n",
        "    if history['val_loss']:\n",
        "        best_idx = history['val_loss'].index(min(history['val_loss']))\n",
        "    else:\n",
        "        best_idx = -1\n",
        "\n",
        "    final_metrics = {\n",
        "        'train_loss': history['train_loss'][best_idx],\n",
        "        'val_loss': history['val_loss'][best_idx] if history['val_loss'] else None,\n",
        "        'dead_ratio': history['dead_ratio'][best_idx],\n",
        "    }\n",
        "\n",
        "    # R² at each k level (from best epoch)\n",
        "    for k in CONFIG['k_levels']:\n",
        "        key = f'r2_k{k}'\n",
        "        if key in history['val_r2'] and history['val_r2'][key]:\n",
        "            final_metrics[key] = history['val_r2'][key][best_idx]\n",
        "\n",
        "    # Clean up\n",
        "    del model, trainer, train_loader, val_loader\n",
        "    clear_memory(verbose=True)\n",
        "\n",
        "    return final_metrics, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-9",
        "outputId": "8df23bcf-0085-426e-e66e-1c1e8af753e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training MSAE for Block 5\n",
            "============================================================\n",
            "\n",
            "=== System Capabilities ===\n",
            "CPU cores: 8\n",
            "Total RAM: 51.0 GB\n",
            "Available RAM: 48.9 GB\n",
            "Memory budget: 50%\n",
            "GPU: Tesla T4 (14.7 GB)\n",
            "Auto-detected optimal workers: 7\n",
            "Using HDF5 streaming from: /content/drive/MyDrive/chaos/data/activations.h5\n",
            "Using chunked streaming: 495,995 samples\n",
            "  Train: 446,396, Val: 49,599\n",
            "  Chunk size: 100,000\n",
            "Model parameters: 2,097,408\n",
            "Training MSAE on cuda\n",
            "  K levels: [16, 32, 64, 128]\n",
            "  Hidden dim: 4096\n",
            "  Learning rate: 0.0008\n",
            "  Epochs: 30\n",
            "\n",
            "Epoch 1/30\n",
            "  Batch 50/109: loss=0.822782, dead=0.000\n",
            "  Batch 100/109: loss=0.595525, dead=0.000\n",
            "  Batch 150/109: loss=0.485413, dead=0.000\n",
            "  Batch 200/109: loss=0.415995, dead=0.000\n",
            "  Batch 250/109: loss=0.369041, dead=0.000\n",
            "  Batch 300/109: loss=0.336191, dead=0.000\n",
            "  Batch 350/109: loss=0.310493, dead=0.000\n",
            "  Batch 400/109: loss=0.289900, dead=0.000\n",
            "  Batch 450/109: loss=0.273528, dead=0.000\n",
            "  Batch 500/109: loss=0.259915, dead=0.000\n",
            "  Batch 550/109: loss=0.248005, dead=0.000\n",
            "  Batch 600/109: loss=0.237742, dead=0.000\n",
            "  Batch 650/109: loss=0.229004, dead=0.000\n",
            "  Batch 700/109: loss=0.221070, dead=0.000\n",
            "  Batch 750/109: loss=0.213934, dead=0.000\n",
            "  Time: 44.9s\n",
            "  Train loss: 0.212189 (rec: 0.212189, aux: 0.000000)\n",
            "  Dead latents: 0.000\n",
            "  Val loss: 0.120426\n",
            "  Val R²: k=16: 0.7873, k=32: 0.8648, k=64: 0.9208, k=128: 0.9525\n",
            "  Saved checkpoint: /content/drive/MyDrive/chaos/checkpoints/msae_epoch1.pt\n",
            "  Saved best model: /content/drive/MyDrive/chaos/models/msae_best.pt\n",
            "\n",
            "Epoch 2/30\n",
            "  Batch 50/109: loss=0.111963, dead=0.000\n",
            "  Batch 100/109: loss=0.110738, dead=0.000\n",
            "  Batch 150/109: loss=0.110062, dead=0.000\n",
            "  Batch 200/109: loss=0.108925, dead=0.000\n",
            "  Batch 250/109: loss=0.107931, dead=0.000\n",
            "  Batch 300/109: loss=0.107477, dead=0.000\n",
            "  Batch 350/109: loss=0.106683, dead=0.000\n",
            "  Batch 400/109: loss=0.105826, dead=0.000\n",
            "  Batch 450/109: loss=0.105306, dead=0.000\n",
            "  Batch 500/109: loss=0.104847, dead=0.000\n",
            "  Batch 550/109: loss=0.104235, dead=0.000\n",
            "  Batch 600/109: loss=0.103723, dead=0.000\n",
            "  Batch 650/109: loss=0.103384, dead=0.000\n",
            "  Batch 700/109: loss=0.102951, dead=0.000\n",
            "  Batch 750/109: loss=0.102504, dead=0.000\n",
            "  Time: 44.9s\n",
            "  Train loss: 0.102378 (rec: 0.102378, aux: 0.000000)\n",
            "  Dead latents: 0.000\n",
            "  Val loss: 0.104189\n",
            "  Val R²: k=16: 0.8091, k=32: 0.8806, k=64: 0.9339, k=128: 0.9664\n",
            "  Saved checkpoint: /content/drive/MyDrive/chaos/checkpoints/msae_epoch2.pt\n",
            "  Saved best model: /content/drive/MyDrive/chaos/models/msae_best.pt\n",
            "\n",
            "Epoch 3/30\n",
            "  Batch 50/109: loss=0.096652, dead=0.000\n",
            "  Batch 100/109: loss=0.096410, dead=0.000\n"
          ]
        }
      ],
      "source": [
        "# Train MSAE for each block\n",
        "all_results = {}\n",
        "all_histories = {}\n",
        "\n",
        "for block_idx in CONFIG['block_indices']:\n",
        "    block_dir = activations_dir / f'block{block_idx}'\n",
        "    if not block_dir.exists():\n",
        "        print(f\"Skipping block {block_idx} - no activations found\")\n",
        "        continue\n",
        "\n",
        "    metrics, history = train_msae_for_block(block_idx)\n",
        "    all_results[f'block{block_idx}'] = metrics\n",
        "    all_histories[f'block{block_idx}'] = history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {
        "id": "cell-10"
      },
      "source": [
        "## 3. Summary Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "outputs": [],
      "source": [
        "print(\"\\nFinal Results:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for block_name, metrics in all_results.items():\n",
        "    print(f\"\\n{block_name}:\")\n",
        "    print(f\"  Train Loss: {metrics['train_loss']:.6f}\")\n",
        "    if metrics['val_loss']:\n",
        "        print(f\"  Val Loss: {metrics['val_loss']:.6f}\")\n",
        "    print(f\"  Dead Ratio: {metrics['dead_ratio']:.3f}\")\n",
        "    print(f\"  R² by k:\")\n",
        "    for k in CONFIG['k_levels']:\n",
        "        key = f'r2_k{k}'\n",
        "        if key in metrics:\n",
        "            print(f\"    k={k:3d}: {metrics[key]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12"
      },
      "outputs": [],
      "source": [
        "# Save results summary\n",
        "results_path = Path(CONFIG['output_dir']) / 'results' / 'reconstruction.json'\n",
        "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Format for easy reading\n",
        "reconstruction_results = {}\n",
        "for block_name, metrics in all_results.items():\n",
        "    reconstruction_results[block_name] = {\n",
        "        f'k{k}': metrics.get(f'r2_k{k}', None) for k in CONFIG['k_levels']\n",
        "    }\n",
        "\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(reconstruction_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved results to {results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## 4. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {
        "id": "cell-14"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "colors = {'block5': 'C0', 'block20': 'C1', 'block35': 'C2'}\n",
        "\n",
        "# Plot 1: Training Loss\n",
        "ax = axes[0, 0]\n",
        "for block_name, history in all_histories.items():\n",
        "    ax.plot(history['train_loss'], label=block_name, color=colors.get(block_name))\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Training Loss')\n",
        "ax.set_title('Training Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Validation Loss\n",
        "ax = axes[0, 1]\n",
        "for block_name, history in all_histories.items():\n",
        "    if history['val_loss']:\n",
        "        ax.plot(history['val_loss'], label=block_name, color=colors.get(block_name))\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Loss')\n",
        "ax.set_title('Validation Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Dead Latent Ratio\n",
        "ax = axes[1, 0]\n",
        "for block_name, history in all_histories.items():\n",
        "    ax.plot(history['dead_ratio'], label=block_name, color=colors.get(block_name))\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Dead Latent Ratio')\n",
        "ax.set_title('Dead Latent Ratio')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: R² at different k levels (final epoch)\n",
        "ax = axes[1, 1]\n",
        "for block_name, metrics in all_results.items():\n",
        "    r2_values = [metrics.get(f'r2_k{k}', 0) for k in CONFIG['k_levels']]\n",
        "    ax.plot(CONFIG['k_levels'], r2_values, 'o-', label=block_name, color=colors.get(block_name))\n",
        "ax.set_xlabel('k (sparsity level)')\n",
        "ax.set_ylabel('R²')\n",
        "ax.set_title('Final R² by Sparsity Level')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(CONFIG['output_dir']) / 'figures' / 'msae_training.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "## 5. Check Success Criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {
        "id": "cell-16"
      },
      "outputs": [],
      "source": [
        "# Success criteria from CLAUDE.md\n",
        "print(\"\\nSuccess Criteria Check:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "criteria = {\n",
        "    'R² (k=128) > 0.85': [],\n",
        "    'R² (k=16) > 0.40': [],\n",
        "    'Dead features < 30%': [],\n",
        "}\n",
        "\n",
        "for block_name, metrics in all_results.items():\n",
        "    r2_128 = metrics.get('r2_k128', 0)\n",
        "    r2_16 = metrics.get('r2_k16', 0)\n",
        "    dead = metrics.get('dead_ratio', 1)\n",
        "\n",
        "    criteria['R² (k=128) > 0.85'].append((block_name, r2_128, r2_128 > 0.85))\n",
        "    criteria['R² (k=16) > 0.40'].append((block_name, r2_16, r2_16 > 0.40))\n",
        "    criteria['Dead features < 30%'].append((block_name, dead, dead < 0.30))\n",
        "\n",
        "for criterion, results in criteria.items():\n",
        "    print(f\"\\n{criterion}:\")\n",
        "    for block, value, passed in results:\n",
        "        status = \"PASS\" if passed else \"FAIL\"\n",
        "        print(f\"  {block}: {value:.4f} [{status}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ndBa8sVeOl_",
      "metadata": {
        "id": "3ndBa8sVeOl_"
      },
      "outputs": [],
      "source": [
        "# Zip the essential outputs\n",
        "!zip -r msae_models.zip outputs/models/ outputs/results/ outputs/figures/\n",
        "\n",
        "# Then download msae_models.zip (~25MB)\n",
        "from google.colab import files\n",
        "files.download('msae_models.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Trained MSAEs saved to:\n",
        "- `outputs/models/msae_block5.pt`\n",
        "- `outputs/models/msae_block20.pt`\n",
        "- `outputs/models/msae_block35.pt`\n",
        "\n",
        "Each model file includes:\n",
        "- Model state dict\n",
        "- Configuration (k_levels, hidden_dim, etc.)\n",
        "- Normalization statistics (mean, std)\n",
        "- Training history\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **03_train_baselines.ipynb**: Train single-k baseline SAEs for comparison\n",
        "2. **04_concept_labeling.ipynb**: Create concept labels for Go positions\n",
        "3. **05_run_probes.ipynb**: Train linear probes to evaluate feature quality"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
